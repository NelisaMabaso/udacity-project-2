{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponsePipeline.db')\n",
    "df = pd.read_sql('SELECT * FROM DisasterResponse', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "        \n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\",\" \",text.lower().strip())\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "y = df.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(clf):\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_transform', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ]))\n",
    "        ])),\n",
    "        ('clf', MultiOutputClassifier(clf))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf_random_forest = pipeline(clf=RandomForestClassifier())\n",
    "clf_random_forest.fit(X_train, y_train)\n",
    "y_pred_clf_random_forest = clf_random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  16.21375436465766\n",
      "The recall is  29.146896484942385\n",
      "The precision is  41.690876728384715\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.77      0.97      0.86      5018\n",
      "               request       0.45      0.06      0.10      1103\n",
      "                 offer       0.00      0.00      0.00        31\n",
      "           aid_related       0.46      0.22      0.29      2680\n",
      "          medical_help       0.10      0.00      0.01       523\n",
      "      medical_products       0.08      0.01      0.01       288\n",
      "     search_and_rescue       0.00      0.00      0.00       186\n",
      "              security       0.00      0.00      0.00       129\n",
      "              military       0.00      0.00      0.00       210\n",
      "                 water       0.08      0.00      0.00       423\n",
      "                  food       0.25      0.01      0.03       740\n",
      "               shelter       0.13      0.01      0.02       548\n",
      "              clothing       0.20      0.01      0.02       111\n",
      "                 money       0.00      0.00      0.00       161\n",
      "        missing_people       0.00      0.00      0.00        86\n",
      "              refugees       0.14      0.01      0.01       200\n",
      "                 death       0.00      0.00      0.00       277\n",
      "             other_aid       0.09      0.01      0.01       866\n",
      "infrastructure_related       0.11      0.01      0.01       436\n",
      "             transport       0.00      0.00      0.00       303\n",
      "             buildings       0.00      0.00      0.00       292\n",
      "           electricity       0.00      0.00      0.00       137\n",
      "                 tools       0.00      0.00      0.00        42\n",
      "             hospitals       0.00      0.00      0.00        79\n",
      "                 shops       0.00      0.00      0.00        36\n",
      "           aid_centers       0.00      0.00      0.00        79\n",
      "  other_infrastructure       0.17      0.00      0.01       284\n",
      "       weather_related       0.54      0.17      0.26      1817\n",
      "                floods       0.24      0.01      0.02       547\n",
      "                 storm       0.51      0.04      0.07       614\n",
      "                  fire       0.00      0.00      0.00        68\n",
      "            earthquake       0.64      0.14      0.23       614\n",
      "                  cold       0.00      0.00      0.00       115\n",
      "         other_weather       0.25      0.00      0.01       348\n",
      "         direct_report       0.43      0.04      0.08      1263\n",
      "\n",
      "             micro avg       0.67      0.29      0.41     20654\n",
      "             macro avg       0.16      0.05      0.06     20654\n",
      "          weighted avg       0.42      0.29      0.29     20654\n",
      "           samples avg       0.68      0.36      0.40     20654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy is \", accuracy_score(y_test,y_pred_clf_random_forest)*100)\n",
    "print(\"The recall is \", recall_score(y_test,y_pred_clf_random_forest, average = 'weighted')*100)\n",
    "print(\"The precision is \", precision_score(y_test,y_pred_clf_random_forest, average = 'weighted')*100)\n",
    "\n",
    "print(classification_report(y_test, y_pred_clf_random_forest,digits=2,target_names=y.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('features',\n",
       "   FeatureUnion(transformer_list=[('text_transform',\n",
       "                                   Pipeline(steps=[('vect',\n",
       "                                                    CountVectorizer(tokenizer=<function tokenize at 0x000001ACB852FC40>)),\n",
       "                                                   ('tfidf',\n",
       "                                                    TfidfTransformer())]))])),\n",
       "  ('clf', MultiOutputClassifier(estimator=RandomForestClassifier()))],\n",
       " 'verbose': False,\n",
       " 'features': FeatureUnion(transformer_list=[('text_transform',\n",
       "                                 Pipeline(steps=[('vect',\n",
       "                                                  CountVectorizer(tokenizer=<function tokenize at 0x000001ACB852FC40>)),\n",
       "                                                 ('tfidf',\n",
       "                                                  TfidfTransformer())]))]),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier()),\n",
       " 'features__n_jobs': None,\n",
       " 'features__transformer_list': [('text_transform',\n",
       "   Pipeline(steps=[('vect',\n",
       "                    CountVectorizer(tokenizer=<function tokenize at 0x000001ACB852FC40>)),\n",
       "                   ('tfidf', TfidfTransformer())]))],\n",
       " 'features__transformer_weights': None,\n",
       " 'features__verbose': False,\n",
       " 'features__text_transform': Pipeline(steps=[('vect',\n",
       "                  CountVectorizer(tokenizer=<function tokenize at 0x000001ACB852FC40>)),\n",
       "                 ('tfidf', TfidfTransformer())]),\n",
       " 'features__text_transform__memory': None,\n",
       " 'features__text_transform__steps': [('vect',\n",
       "   CountVectorizer(tokenizer=<function tokenize at 0x000001ACB852FC40>)),\n",
       "  ('tfidf', TfidfTransformer())],\n",
       " 'features__text_transform__verbose': False,\n",
       " 'features__text_transform__vect': CountVectorizer(tokenizer=<function tokenize at 0x000001ACB852FC40>),\n",
       " 'features__text_transform__tfidf': TfidfTransformer(),\n",
       " 'features__text_transform__vect__analyzer': 'word',\n",
       " 'features__text_transform__vect__binary': False,\n",
       " 'features__text_transform__vect__decode_error': 'strict',\n",
       " 'features__text_transform__vect__dtype': numpy.int64,\n",
       " 'features__text_transform__vect__encoding': 'utf-8',\n",
       " 'features__text_transform__vect__input': 'content',\n",
       " 'features__text_transform__vect__lowercase': True,\n",
       " 'features__text_transform__vect__max_df': 1.0,\n",
       " 'features__text_transform__vect__max_features': None,\n",
       " 'features__text_transform__vect__min_df': 1,\n",
       " 'features__text_transform__vect__ngram_range': (1, 1),\n",
       " 'features__text_transform__vect__preprocessor': None,\n",
       " 'features__text_transform__vect__stop_words': None,\n",
       " 'features__text_transform__vect__strip_accents': None,\n",
       " 'features__text_transform__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'features__text_transform__vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'features__text_transform__vect__vocabulary': None,\n",
       " 'features__text_transform__tfidf__norm': 'l2',\n",
       " 'features__text_transform__tfidf__smooth_idf': True,\n",
       " 'features__text_transform__tfidf__sublinear_tf': False,\n",
       " 'features__text_transform__tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'sqrt',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__monotonic_cst': None,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_random_forest.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used Random Search instead of Grid Search to save some computational costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:318: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__estimator__min_samples_split': 3, 'clf__estimator__max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "clf_random_forest = pipeline(RandomForestClassifier())\n",
    "\n",
    "# Define the parameter distribution for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'clf__estimator__max_depth': [None, 10, 20],         # Maximum depth of each tree\n",
    "    'clf__estimator__min_samples_split': [2, 3, 4]                 # Minimum samples required to split a node\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with the pipeline and randomized parameter search\n",
    "clf_random_forest_cv = RandomizedSearchCV(clf_random_forest, param_distributions=param_distributions, \n",
    "                                          cv=2, n_iter=10, random_state=42)\n",
    "\n",
    "# Fit the randomized search to the training data\n",
    "clf_random_forest_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best Parameters:\", clf_random_forest_cv.best_params_)\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred_clf_random_forest_cv = clf_random_forest_cv.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  20.494914224988612\n",
      "The recall is  24.5424615086666\n",
      "The precision is  29.15888293187074\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related     0.7618    1.0000    0.8648      5018\n",
      "               request     0.0000    0.0000    0.0000      1103\n",
      "                 offer     0.0000    0.0000    0.0000        31\n",
      "           aid_related     0.0000    0.0000    0.0000      2680\n",
      "          medical_help     0.0000    0.0000    0.0000       523\n",
      "      medical_products     0.0000    0.0000    0.0000       288\n",
      "     search_and_rescue     0.0000    0.0000    0.0000       186\n",
      "              security     0.0000    0.0000    0.0000       129\n",
      "              military     0.0000    0.0000    0.0000       210\n",
      "                 water     0.0000    0.0000    0.0000       423\n",
      "                  food     0.0000    0.0000    0.0000       740\n",
      "               shelter     0.0000    0.0000    0.0000       548\n",
      "              clothing     0.0000    0.0000    0.0000       111\n",
      "                 money     0.0000    0.0000    0.0000       161\n",
      "        missing_people     0.0000    0.0000    0.0000        86\n",
      "              refugees     0.0000    0.0000    0.0000       200\n",
      "                 death     0.0000    0.0000    0.0000       277\n",
      "             other_aid     0.0000    0.0000    0.0000       866\n",
      "infrastructure_related     0.0000    0.0000    0.0000       436\n",
      "             transport     0.0000    0.0000    0.0000       303\n",
      "             buildings     0.0000    0.0000    0.0000       292\n",
      "           electricity     0.0000    0.0000    0.0000       137\n",
      "                 tools     0.0000    0.0000    0.0000        42\n",
      "             hospitals     0.0000    0.0000    0.0000        79\n",
      "                 shops     0.0000    0.0000    0.0000        36\n",
      "           aid_centers     0.0000    0.0000    0.0000        79\n",
      "  other_infrastructure     0.0000    0.0000    0.0000       284\n",
      "       weather_related     0.8727    0.0264    0.0513      1817\n",
      "                floods     0.0000    0.0000    0.0000       547\n",
      "                 storm     0.0000    0.0000    0.0000       614\n",
      "                  fire     0.0000    0.0000    0.0000        68\n",
      "            earthquake     1.0000    0.0049    0.0097       614\n",
      "                  cold     0.0000    0.0000    0.0000       115\n",
      "         other_weather     0.0000    0.0000    0.0000       348\n",
      "         direct_report     0.0000    0.0000    0.0000      1263\n",
      "\n",
      "             micro avg     0.7628    0.2454    0.3714     20654\n",
      "             macro avg     0.0753    0.0295    0.0265     20654\n",
      "          weighted avg     0.2916    0.2454    0.2149     20654\n",
      "           samples avg     0.7614    0.3330    0.4074     20654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy is \", accuracy_score(y_test,y_pred_clf_random_forest_cv)*100)\n",
    "print(\"The recall is \", recall_score(y_test,y_pred_clf_random_forest_cv, average = 'weighted')*100)\n",
    "print(\"The precision is \", precision_score(y_test,y_pred_clf_random_forest_cv, average = 'weighted')*100)\n",
    "\n",
    "print(classification_report(y_test, y_pred_clf_random_forest_cv,digits=4,target_names=y.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(word_tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_2(clf):\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_transform', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "        ('clf', MultiOutputClassifier(clf))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  17.23090936693487\n",
      "The recall is  28.696620509344438\n",
      "The precision is  41.418767538586785\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related     0.7663    0.9843    0.8617      5018\n",
      "               request     0.4069    0.1287    0.1956      1103\n",
      "                 offer     0.0000    0.0000    0.0000        31\n",
      "           aid_related     0.4852    0.0978    0.1627      2680\n",
      "          medical_help     0.0000    0.0000    0.0000       523\n",
      "      medical_products     0.0000    0.0000    0.0000       288\n",
      "     search_and_rescue     0.0000    0.0000    0.0000       186\n",
      "              security     0.0000    0.0000    0.0000       129\n",
      "              military     0.0000    0.0000    0.0000       210\n",
      "                 water     0.2500    0.0024    0.0047       423\n",
      "                  food     0.3750    0.0162    0.0311       740\n",
      "               shelter     0.3333    0.0018    0.0036       548\n",
      "              clothing     0.1250    0.0090    0.0168       111\n",
      "                 money     0.0000    0.0000    0.0000       161\n",
      "        missing_people     0.0000    0.0000    0.0000        86\n",
      "              refugees     0.0000    0.0000    0.0000       200\n",
      "                 death     0.0000    0.0000    0.0000       277\n",
      "             other_aid     0.0000    0.0000    0.0000       866\n",
      "infrastructure_related     0.0000    0.0000    0.0000       436\n",
      "             transport     0.0000    0.0000    0.0000       303\n",
      "             buildings     0.0000    0.0000    0.0000       292\n",
      "           electricity     0.0000    0.0000    0.0000       137\n",
      "                 tools     0.0000    0.0000    0.0000        42\n",
      "             hospitals     0.0000    0.0000    0.0000        79\n",
      "                 shops     0.0000    0.0000    0.0000        36\n",
      "           aid_centers     0.5000    0.0127    0.0247        79\n",
      "  other_infrastructure     0.0000    0.0000    0.0000       284\n",
      "       weather_related     0.5423    0.1871    0.2782      1817\n",
      "                floods     0.1429    0.0018    0.0036       547\n",
      "                 storm     0.5149    0.0847    0.1455       614\n",
      "                  fire     0.0000    0.0000    0.0000        68\n",
      "            earthquake     0.6789    0.1205    0.2047       614\n",
      "                  cold     0.0000    0.0000    0.0000       115\n",
      "         other_weather     0.2000    0.0029    0.0057       348\n",
      "         direct_report     0.3759    0.0792    0.1308      1263\n",
      "\n",
      "             micro avg     0.6891    0.2870    0.4052     20654\n",
      "             macro avg     0.1628    0.0494    0.0591     20654\n",
      "          weighted avg     0.4142    0.2870    0.2855     20654\n",
      "           samples avg     0.7028    0.3565    0.4096     20654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 2. Ada Boost Classifier\n",
    "\n",
    "clf_ada = pipeline_2(clf=AdaBoostClassifier(n_estimators=50, random_state=42))\n",
    "clf_ada.fit(X_train, y_train)\n",
    "y_pred_clf_ada = clf_ada.predict(X_test)\n",
    "\n",
    "print(\"accuracy is \", accuracy_score(y_test,y_pred_clf_ada)*100)\n",
    "print(\"The recall is \", recall_score(y_test,y_pred_clf_ada, average = 'weighted')*100)\n",
    "print(\"The precision is \", precision_score(y_test,y_pred_clf_ada, average = 'weighted')*100)\n",
    "\n",
    "print(classification_report(y_test, y_pred_clf_ada,digits=4,target_names=y.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  16.259298618490966\n",
      "The recall is  30.10070688486492\n",
      "The precision is  42.47060533692547\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related     0.7654    0.9845    0.8612      5018\n",
      "               request     0.4260    0.1278    0.1967      1103\n",
      "                 offer     0.0000    0.0000    0.0000        31\n",
      "           aid_related     0.4513    0.1832    0.2606      2680\n",
      "          medical_help     0.0000    0.0000    0.0000       523\n",
      "      medical_products     0.3333    0.0035    0.0069       288\n",
      "     search_and_rescue     0.0000    0.0000    0.0000       186\n",
      "              security     0.0000    0.0000    0.0000       129\n",
      "              military     0.0000    0.0000    0.0000       210\n",
      "                 water     0.0000    0.0000    0.0000       423\n",
      "                  food     0.3548    0.0149    0.0285       740\n",
      "               shelter     0.0000    0.0000    0.0000       548\n",
      "              clothing     0.5000    0.0180    0.0348       111\n",
      "                 money     0.0000    0.0000    0.0000       161\n",
      "        missing_people     0.0000    0.0000    0.0000        86\n",
      "              refugees     0.0000    0.0000    0.0000       200\n",
      "                 death     0.0000    0.0000    0.0000       277\n",
      "             other_aid     0.3333    0.0023    0.0046       866\n",
      "infrastructure_related     0.0000    0.0000    0.0000       436\n",
      "             transport     0.0000    0.0000    0.0000       303\n",
      "             buildings     0.0000    0.0000    0.0000       292\n",
      "           electricity     0.0000    0.0000    0.0000       137\n",
      "                 tools     0.0000    0.0000    0.0000        42\n",
      "             hospitals     0.0000    0.0000    0.0000        79\n",
      "                 shops     0.0000    0.0000    0.0000        36\n",
      "           aid_centers     0.0000    0.0000    0.0000        79\n",
      "  other_infrastructure     0.0000    0.0000    0.0000       284\n",
      "       weather_related     0.5574    0.1871    0.2802      1817\n",
      "                floods     0.3571    0.0091    0.0178       547\n",
      "                 storm     0.5133    0.0945    0.1596       614\n",
      "                  fire     0.0000    0.0000    0.0000        68\n",
      "            earthquake     0.7540    0.1547    0.2568       614\n",
      "                  cold     0.0000    0.0000    0.0000       115\n",
      "         other_weather     0.0000    0.0000    0.0000       348\n",
      "         direct_report     0.4456    0.1037    0.1683      1263\n",
      "\n",
      "             micro avg     0.6825    0.3010    0.4178     20654\n",
      "             macro avg     0.1655    0.0538    0.0650     20654\n",
      "          weighted avg     0.4247    0.3010    0.3028     20654\n",
      "           samples avg     0.6956    0.3658    0.4149     20654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nelisa.mabaso_takeal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 3. Extreme Gradient Boosting Classifier\n",
    "\n",
    "clf_XGB = pipeline_2(XGBClassifier())\n",
    "clf_XGB.fit(X_train, y_train)\n",
    "y_pred_clf_XGB = clf_XGB.predict(X_test)\n",
    "\n",
    "print(\"accuracy is \", accuracy_score(y_test,y_pred_clf_XGB)*100)\n",
    "print(\"The recall is \", recall_score(y_test,y_pred_clf_XGB, average = 'weighted')*100)\n",
    "print(\"The precision is \", precision_score(y_test,y_pred_clf_XGB, average = 'weighted')*100)\n",
    "\n",
    "print(classification_report(y_test, y_pred_clf_XGB,digits=4,target_names=y.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(clf_XGB, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
